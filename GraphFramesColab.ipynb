{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Xbh5SqjMmKy_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25c1562a-a50f-4d35-ea35-62d125c50177"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 317.2/317.2 MB 4.5 MB/s eta 0:00:00\n",
            "✅ Spark 3.5.5 installed in /opt/spark\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "+ SPARK_VERSION=3.5.5\n",
            "+ HADOOP_VER=3\n",
            "+ DL_BASE=https://dlcdn.apache.org/spark/spark-3.5.5\n",
            "+ apt-get update -qq\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "+ apt-get install -y openjdk-11-jdk-headless\n",
            "+ wget -q https://dlcdn.apache.org/spark/spark-3.5.5/spark-3.5.5-bin-hadoop3.tgz\n",
            "+ tar -xf spark-3.5.5-bin-hadoop3.tgz\n",
            "+ mv spark-3.5.5-bin-hadoop3 /opt/spark\n",
            "+ pip install -q pyspark==3.5.5 findspark\n",
            "+ rm spark-3.5.5-bin-hadoop3.tgz\n",
            "+ echo '✅ Spark 3.5.5 installed in /opt/spark'\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "set -euxo pipefail\n",
        "\n",
        "# ── Configuration ───────────────────────────────────────────\n",
        "SPARK_VERSION=3.5.5       # latest patch in 3.5 line (May 2025)\n",
        "HADOOP_VER=3              # Spark 3.5 ships only with Hadoop 3 builds\n",
        "DL_BASE=https://dlcdn.apache.org/spark/spark-${SPARK_VERSION}\n",
        "\n",
        "# ── 1. OS-level deps: Java 11 ───────────────────────────────\n",
        "apt-get update -qq\n",
        "apt-get install -y openjdk-11-jdk-headless > /dev/null\n",
        "\n",
        "# ── 2. Download + unpack Spark ──────────────────────────────\n",
        "wget -q ${DL_BASE}/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VER}.tgz\n",
        "tar -xf spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VER}.tgz\n",
        "mv spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VER} /opt/spark\n",
        "\n",
        "# ── 3. Python bindings & utility helpers ────────────────────\n",
        "pip install -q pyspark==${SPARK_VERSION} findspark\n",
        "\n",
        "# ── 4. (optional) cleanup tarball to save space ─────────────\n",
        "rm spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VER}.tgz\n",
        "\n",
        "echo \"✅ Spark ${SPARK_VERSION} installed in /opt/spark\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, findspark\n",
        "os.environ[\"JAVA_HOME\"]  = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/opt/spark\"\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = (\n",
        "    SparkSession.builder\n",
        "    .master(\"local[*]\")\n",
        "    .config(\"spark.jars.packages\",\n",
        "            \"graphframes:graphframes:0.8.4-spark3.5-s_2.12\")\n",
        "    .getOrCreate()\n",
        ")\n",
        "\n",
        "print(\"Spark version:\", spark.version)\n"
      ],
      "metadata": {
        "id": "FOg0rw1VmPOS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9193b4c-a376-43a0-a8c7-b893c5b91853"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark version: 3.5.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Quick smoke test\n",
        "from graphframes import GraphFrame\n",
        "v = spark.createDataFrame([(\"a\",), (\"b\",)], [\"id\"])\n",
        "e = spark.createDataFrame([(\"a\", \"b\")], [\"src\", \"dst\"])\n",
        "GraphFrame(v, e).inDegrees.show()"
      ],
      "metadata": {
        "id": "-eNcz3O_mUSY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ef81c92-9c8d-49a3-e25f-a48725d86758"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------+\n",
            "| id|inDegree|\n",
            "+---+--------+\n",
            "|  b|       1|\n",
            "+---+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vzulaaDsUaMs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}